---
layout: single
title:  "[iOS] Core ML"
date:   2021-08-08 2:54:50 +0900
categories: iOS
tags: [Machine Learning]
toc: true
toc_sticky: true
header:
  overlay_image: /images/background3.jpg
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
---

이번 포스팅에서는 iOS앱에서 인공지능 모델을 활용할 수 있도록 도와주는 프레임워크인 Core ML 문서를 번역해 보겠습니다.

<span style="color:gray">참고자료 : <br></span><a href ="https://developer.apple.com/documentation/coreml" style="color:darkgreen"><U>Core ML 공식문서</U></a>

## **Core ML이란?**

Core ML이란 iOS앱에서 Machine Learning 모델을 사용할 수 있도록 도와주는 프레임워크입니다.

여러 인공지능 모델에 대한 적용이 용이하도록 Unified Representation을 제공합니다.

앱은 Core ML API 와 user data를 활용하여 모델을 학습시키고 최적화하며 input Data에 대한 결과값 예측도 **기기 자체**에서 가능하게합니다.

<br>
<p align="center"><img width="500" alt="4b0ecf58-a51a-4bfa-a361-eb77e59ed76e" src="https://user-images.githubusercontent.com/56648865/128632445-5b0a48a2-23ce-4d97-ac27-a8e96ff6bd68.png"></p>

하나의 모델은 학습데이터를 하나의 머신러닝 알고리즘을 이용하여 학습시킨 결과물을 말합니다. 

학습이 끝난 모델에 새로운 데이터를 주입해주면 데이터에 대한 예측치를 반환합니다.

학습된 모델은 이미지를 분류하거나 이미지 상에서 특정 오브젝트를 detect하는 코드로써는 구현하기 힘든 부분을 가능하게 합니다.

<br>
Xcode에서는 `Create ML app`이라는 번들을 활용하여 모델을 구축하고 학습시킬 수 있습니다. 

Create ML로 학습된 모델은 Core ML 모델 안에 존재하며 앱 내에서 사용될 수 있습니다.

이외에도 Create ML 뿐만 아니라 다른 machine learning 라이브러리(케라스, 텐서플로우, 파이토치)등을 활용하여 학습시키고 학습된 모델을 `Core ML Tools`를 활용하여 Core ML 형식으로 변환해줄 수 있습니다.

한번 Core ML형식으로 기기안에서 동작될 수 있는 모델은 다른 데이터를 활용하여 재학습시키거나 정확도를 높이기 위해 최적화 시킬 수 있습니다.

<br>
Core ML은 CPU, GPU, Neural Engine을 활용하여 on-device performance를 최적화시킵니다.

또한 네트워크와의 연동 없이 동작하여 데이터를 보호하고 네트워크가 없는 상황에서도 사용할 수 있습니다.

<br>
Core ML은 CV, NLP, Speech, Sound Analysis와 같이 여러 인공신경망 모델에 적용될 수 있습니다.

또한  Core ML은 `BNNS`(인공 신경망을 실행시키며 CPU효율을 관리하는 프레임워크)와 `Metal Performance Shaders`(GPU를 활용하여 이미지의 특징을 추출하고 인공신경망을 실행시키는 프레임워크)와 같은 저수준 기본요소 위에서 구축됩니다.

<p align="center"><img width="500" alt="e3663268-5db4-42c9-a7f0-2114920a9f1f" src="https://user-images.githubusercontent.com/56648865/128634795-fb7b4241-aa86-47b5-b4db-5b2645338566.png"></p>


<br><br>
## **Core ML API**

Xcode에 모델을 추가할 때 자동적으로 구축된 인터페이스를 통해서 모델과 모델 사용자(개발자,나)간에 소통을 할 수 있습니다.

이 때 사용하는 것이 Core ML API인데 이를 통해 우리는 모델의 학습 또는 예측된 결과를 비동기적으로 사용할 수 있습니다.

Core ML API를 사용하기 위해서는 
    - 타입에 MLFeatureProvider 프로토콜 채택
    - MLFeatureProvider를 채택한 타입 내에서 MLModel 메서드 사용
을 해주어야 합니다.

<br><br>
## **Create ML**

Swift 나 MacOS 플레이그라운드와 같은 툴에서 인공지능 모델을 설계하고 훈련시킬 수 있도록 하는 프레임워크입니다.

<br>
<p align="center"><img width="500" alt="ef8fb04e-9aa3-4068-b765-b4818e482168" src="https://user-images.githubusercontent.com/56648865/128637199-7ec83ffb-68f9-4067-b780-d10cd5c5bbcf.png"></p>

Create ML로 테스트까지 마친 학습된 모델을 앱에 사용하기 위해서는 Core ML을 통해서 앱에 합쳐줘야 합니다.

<br>
<p align="center"><img width="300" alt="01b8bfa4-e352-4a6b-becb-8be6d8e2c45c" src="https://user-images.githubusercontent.com/56648865/128637213-450f51db-fd61-44a1-9c12-f1ce7e5a876d.png"></p>

Create ML은 Machine Learning의 기능을 아이폰 기본 앱인 사진 앱이나 시리에서도 효율적으로 사용할 수 있도록 합니다.

즉 우리가 사용하는 인공지능 모델이 훈련하는데 시간이 많이 들지않고 모델의 크기가 작다는 것을 뜻합니다.